import numpy as np
import pygments.lexers
from gensim.models import Word2Vec
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
import pygments
import sys
sys.path.append('./solidityContracts')
sys.path.append('./contractLabels')
import solidityContracts
import contractLabels

import warnings
warnings.simplefilter('ignore')

def getGlobalVariables(contract):
    # Define a Solidity lexer and use it to get a list of tokens
    lexer = pygments.lexers.get_lexer_by_name('Solidity')
    tokens = list(pygments.lex(contract, lexer))

    # Initialize variables to keep track of brackets and parentheses
    openBracketsCount = 0
    openParenthesisCount = 0

    # Initialize the list of global variables
    globalVarList = []

    # Iterate through the tokens
    for i, token in enumerate(tokens):
        # Skip whitespace and comments
        if str(token[0]) == 'Token.Text.Whitespace' or str(token[0]) == 'Token.Comment.Single':
            None

        # Update the bracket and parentheses counts
        elif token[1] == '{':
            openBracketsCount += 1
        elif token[1] == '}':
            openBracketsCount -= 1
        elif token[1] == '(':
            openParenthesisCount += 1
        elif token[1] == ')':
            openParenthesisCount -= 1

        # If the token is a type keyword and is within the correct brackets and parentheses, extract the potential global variable
        elif (str(token[0]) == 'Token.Keyword.Type' and isType(token[1])
              and openBracketsCount == 1 and openParenthesisCount == 0):
            include = True
            index = 0
            potentialGlobal = ""
            while True:
                if tokens[i + index][1] == "constant":
                    include = False
                if tokens[i + index][1] == ';':
                    if include:
                        globalVarList.append(potentialGlobal)
                    break
                if (tokens[i + index][0] == pygments.token.Name.Variable or tokens[i + index][0] == pygments.token.Text):
                    potentialGlobal = tokens[i + index][1]
                    
                index += 1
            i += (index - 1)

    return globalVarList



def getFunctions(contract):
    lexer = pygments.lexers.get_lexer_by_name('Solidity')
    tokens = list(pygments.lex(contract, lexer))
    functions = []
    i = 0
    while i < len(tokens):
        if tokens[i][1] == 'function':
            function = [tokens[i]]
            i += 1
            
            # Check for one-liner function
            one_liner = False
            while tokens[i][1] != '{':
                if tokens[i][1] == ';':
                    i += 1
                    one_liner = True
                    break
                if tokens[i][0] not in ('Token.Text.Whitespace', 'Token.Comment.Single'):
                    function.append(tokens[i])
                i += 1
                
            function.append(tokens[i])
            open_brackets = 1
            i += 1
            
            # Find the closing brackets for the function
            while open_brackets > 0:
                if tokens[i][1] == '{':
                    open_brackets += 1
                elif tokens[i][1] == '}':
                    open_brackets -= 1
                
                if tokens[i][0] not in ('Token.Text.Whitespace', 'Token.Comment.Single'):
                    function.append(tokens[i])
                i += 1
                
            functions.append(function)
        else:
            i += 1
    return functions

# Extracts names of modifiers (function guards) for tokenization process
def getModifiers(contract):
    # Define a Solidity lexer and use it to get a list of tokens
    lexer = pygments.lexers.get_lexer_by_name('Solidity')
    tokens = list(pygments.lex(contract, lexer))

    # Filter out whitespace tokens
    filteredTokens = []
    for token in tokens:
        if str(token[0]) != 'Token.Text.Whitespace':
            filteredTokens.append(token)

    # Initialize the list of modifier names
    modifierNameList = []

    # Iterate through the filtered tokens
    for i, token in enumerate(filteredTokens):
        # If the token is the "modifier" keyword, add the following token to the list of modifier names
        if (str(token[0]) == 'Token.Keyword.Type' and str(token[1]) == 'modifier'):
            modifierNameList.append(filteredTokens[i + 1][1])

    return modifierNameList

def tokenize(parsedFunction, globalVariablesList, modifierNameList):
     # A list of keywords that indicate a transaction (call, transfer, send)
    txKeywords = ['call', 'transfer', 'send']
    tokenList = []
        # Iterate through the parsed function
    for tup in parsedFunction:
        # Check if the previous token was "function"
        if tokenList and tokenList[-1] == "function":
            tokenList.append("Token.FunctionName")
        # Check if the current token is in the list of transaction keywords
        elif str(tup[1]) in txKeywords:
            tokenList.append("Token.TxWord") 
        # Check if the current token is in the list of global variables
        elif tup[1] in globalVariablesList:
            tokenList.append("Token.GlobalVariable")
        # Check if the current token is in the list of modifier names
        elif tup[1] in modifierNameList:
            tokenList.append("Token.ModifierName")
        # Check if the current token starts with a double quote (indicating a string)   
        elif tup[1].startswith("\""):
            tokenList.append("Token.String")
        # Check if the current token is a local variable or text
        elif str(tup[0]) == 'Token.Name.Variable' or str(tup[0]) == 'Token.Text':
           tokenList.append("Token.LocalVariable")
        # Check if the current token is not whitespace or a single line comment
        elif str(tup[0]) != 'Token.Text.Whitespace' and str(tup[0]) != 'Token.Comment.Single':
            tokenList.append(tup[1])
    return tokenList

# checks if keyword is for variable declaration (helper for global variable parser)
def isType(word):
    keywords = ['address', 'bool', 'byte', 'bytes', 'int', 'string', 'uint', 'mapping']
    for kw in keywords:
        if kw in word:
            return True
    return False


def tokenizeContractFunctions(contract):
    # Get a list of global variables , functions ,modifiers in the contract
    globalVariablesList = getGlobalVariables(contract)
  
    modifierNameList = getModifiers(contract)
   
    functionList = getFunctions(contract)
    
    # Initialize an empty list to store the tokenized functions
    tokenizedFunctionList = []
    for function in functionList:
        tokenizedFunction = tokenize(function, globalVariablesList, modifierNameList)
        tokenizedFunctionList.append(tokenizedFunction)
    
    return tokenizedFunctionList


def returnData(contracts, contractLabels):

    # Initialize empty lists to store the tokenized functions and labels
    functions = []
    labels = []

        # Iterate through the list of contracts
    for i in range(len(contracts)):
        tokenizedFunctions = tokenizeContractFunctions(contracts[i])
        functions.extend(tokenizedFunctions)
        labels.extend(contractLabels[i])
    
    return [functions, labels]



# creates Word2Vec mapping from token to vector

def get_w2v_mapping(tokenizedFunctionsDf, token_dim):
    w2v = Word2Vec(tokenizedFunctionsDf, min_count=1, size=token_dim, workers=3, window=3, sg=1)
    return w2v


# given list of tokenized functions, convert into list of list embeddings

def vectorize_functions(tokenized_functions, w2v_mapping):
    embedding_list = []
    for i in range(len(tokenized_functions)):
        embedding = []
        for token in tokenized_functions[i]:
            embedding.append(w2v_mapping[token])
        
        embedding_list.append(embedding)
    return embedding_list


# given list of embeddings, add padding

def pad_embeddings(embeddings, max_length, token_dim):
    padded_embeddings = []
    for embedding in embeddings:
        zero_padding_cnt = max_length - len(embedding)
        pad = np.zeros((1, token_dim))
        for i in range(zero_padding_cnt):
            embedding = np.concatenate((embedding, pad), axis=0)
        padded_embeddings.append(embedding)
    return padded_embeddings



# compiles Word2Vec mapping, creates embedded function representations, and applies padding

def getFunctionEmbeddings(tokenizedFunctions, max_length, token_dim):
    w2v_mapping = get_w2v_mapping(tokenizedFunctions, token_dim)
    vectorized_fns = vectorize_functions(tokenizedFunctions, w2v_mapping)
    padded_embeddings = pad_embeddings(vectorized_fns, max_length, token_dim)
    return [w2v_mapping, padded_embeddings]


length = 500
token_dim = 15


# Unpack the lists of functions and labels from the returnData function
[functions, labels] = returnData(solidityContracts.contracts, contractLabels.labels)

# Get the function embeddings using the functions and a specified length and token dimension
[w2v_mapping, fn_embeddings] = getFunctionEmbeddings(functions, length, token_dim)

# Convert the function embeddings to a numpy array
fn_embeddings = np.array(fn_embeddings)

print("X shape: ", fn_embeddings.shape)



def tsne_plot(model):

    labels = []
    tokens = []
    #iterating through all the words in the vocab

    for word in model.wv.vocab:
        tokens.append(model[word])
        labels.append(word)
        #initializing the t-SNE model

    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
    new_values = tsne_model.fit_transform(tokens)

    x = []
    y = []
    for value in new_values:
        x.append(value[0])
        y.append(value[1])
            #creating the plot

    plt.figure(figsize=(16,20)) 
    for i in range(len(x)):
        plt.scatter(x[i], y[i])

            #annotating the plot with the words
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(6, 3),
                     textcoords='offset points',
                     ha='right',
                     va='bottom',
                     fontsize=10)
    plt.title('Visualization of Word Vectors')
    plt.show()


functions, labels = returnData(solidityContracts.contracts, contractLabels.labels)

w2v_mapping, fn_embeddings = getFunctionEmbeddings(functions, length, token_dim)

tsne_plot(w2v_mapping)

vocab_size = len(w2v_mapping.wv.vocab)
embedding_dim = 15
embeddings_per_example = 500

# Unpack the lists of functions and labels from the returnData function
[functions, labels] = returnData(solidityContracts.contracts, contractLabels.labels)

# Get the function embeddings using the functions and a specified length and token dimension
[w2v_mapping, X_train] = getFunctionEmbeddings(functions, 500, 15)

# Convert the function embeddings and labels to numpy arrays
X = np.array(X_train)
y = np.array(labels)

# We have 200 contracts as dataset, we split it 40 for test, 160 for training# We have 200 contracts as dataset, 
# We split it 40 for test, 160 for training
X_train, y_train = shuffle(X[40:], y[40:])
X_test = X[:40] 
y_test = y[:40]


def printResults(model, threshold):
    y_pred = model.predict(X_test)
    y_pred_eval = np.argmax(y_pred, axis=1)
    for i in range(len(y_pred)):
        val = 0
        if y_pred[i] > threshold: val = 1

        toPrint = "WRONG"
        if val == y_test[i]: toPrint = "correct"
        print(i, toPrint, ": ", val, " - ", y_pred[i][y_pred_eval[i]])



