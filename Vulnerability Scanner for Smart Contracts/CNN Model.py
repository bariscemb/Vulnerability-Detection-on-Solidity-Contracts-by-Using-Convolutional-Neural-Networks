import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
from sklearn.metrics import classification_report, roc_curve, roc_auc_score, accuracy_score, f1_score, recall_score, precision_score
from keras.models import Sequential
from keras.layers import Dropout
from keras.metrics import  Precision, Recall
from keras import layers

from word2vec import *

import warnings
warnings.simplefilter('ignore')

pd.options.mode.chained_assignment = None 


# Importing evaluation metrics 
metrics = [
    Precision(name='precision'),
    Recall(name='recall')
]


###FOUR LAYER CNN MODEL###

# Computing class weights for balanced classes in the training set
weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_test), y=y_train)
#Defining Model
fourlayer_cnn = Sequential()
#Layer 1 with 128 neurons
fourlayer_cnn.add(layers.Conv1D(128, 10, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]))) 
#Layer 2 with 64 neurons
fourlayer_cnn.add(layers.Conv1D(64, 10, activation='relu'))
#Layer 3 with 32 neurons
fourlayer_cnn.add(layers.Conv1D(32, 10, activation='relu'))
#Layer 4 with 16 neurons
fourlayer_cnn.add(layers.Conv1D(16, 10, activation='relu'))
#Dropout with 0.3 rate to decrease size
fourlayer_cnn.add(Dropout(0.3))
#Pooling layer
fourlayer_cnn.add(layers.GlobalMaxPooling1D()) 
#Output layer with sigmoid activation function
fourlayer_cnn.add(layers.Dense(1, activation='sigmoid')) 

# Compiling the model
fourlayer_cnn.compile(optimizer='adam',
                loss='binary_crossentropy',
                metrics=['accuracy'])

# Training the model
history = fourlayer_cnn.fit(X_train, y_train, validation_data=(X_test, y_test),
        batch_size=30, epochs=150, verbose=0)


# Split data into training, validation, and test sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.20)


y_pred = fourlayer_cnn.predict(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)


outputs = [1 if n >= 0.4 else 0 for n in y_pred]
f1 = f1_score(y_test, outputs, zero_division=1)
recall = recall_score(y_test, outputs, zero_division=1)
precision = precision_score(y_test, outputs, zero_division=1)
accuracy = accuracy_score(y_test, outputs)

# Create a list of the evaluation metric names and values
metrics = [("F1", f1), ("Recall", recall), ("Precision", precision), ("Accuracy", accuracy)]


# Create a bar chart to display the evaluation metric values
plt.bar([x[0] for x in metrics], [x[1] for x in metrics])
plt.show()


outputs = [1 if n >= 0.4 else 0 for n in y_pred]

# Calculate AUC score
roc_auc = roc_auc_score(y_test, y_pred)

# Plot ROC curve
plt.figure(figsize=(5,5))
plt.plot([0, 1], [0, 1], '--', color='gray', label='Random guessing')
plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.3f)' % roc_auc)
plt.xlabel('False positive rate', fontsize=14)
plt.ylabel('True positive rate', fontsize=14)
plt.title('ROC curve', fontsize=16)
plt.legend(loc='best', fontsize=12)
plt.grid(True)
plt.show()


# Plot training and validation loss
plt.figure(figsize=(5,5))
plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.title('Model loss', fontsize=16)
plt.ylabel('Loss', fontsize=14)
plt.xlabel('Epoch', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)
plt.show()

print(classification_report(y_test, outputs))
print("F1 Score: ", f1_score(y_test, outputs, zero_division=1))
print("Recall Score: ", recall_score(y_test, outputs, zero_division=1))
print("Precision Score: ", precision_score(y_test, outputs, zero_division=1))
print("Accuracy Score: ", accuracy_score(y_test, outputs))

